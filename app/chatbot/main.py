import os
import sys
import asyncio
from asyncio import Lock, Event, create_task
from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from app.chatbot.tool_agents.executor.normalanswer import run_final_answer_generation
from app.chatbot.initial_agents.controller import run_initial_controller
from app.chatbot.tool_agents.controller import run_full_consultation
from app.chatbot.tool_agents.utils.utils import faiss_kiwi
from fastapi import FastAPI
from app.chatbot.routes import router as chatbot_router


# âœ… ë½: ì¤‘ë³µ ì‹¤í–‰ ë°©ì§€ (LLM2 ê´€ë ¨)
llm2_lock = Lock()
yes_count = 0
sys.path.append(os.path.abspath("."))
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
DB_FAISS_PATH = "./app/chatbot_term/vectorstore"
app = FastAPI()

def load_faiss():
    try:
        embedding_model = OpenAIEmbeddings(
            model="text-embedding-ada-002",
            openai_api_key=OPENAI_API_KEY,
        )
        return FAISS.load_local(
            DB_FAISS_PATH,
            embedding_model,
            allow_dangerous_deserialization=True,
        )
    except Exception as e:
        # print(f"âŒ FAISS ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None


async def run_dual_pipeline(user_query: str):
    global yes_count
    # print(f"\nğŸ” ì‚¬ìš©ì ì§ˆë¬¸ ìˆ˜ì‹ : {user_query}")

    faiss_db = load_faiss()
    template_data = {}
    if not faiss_db:
        return {"error": "FAISS ë¡œë“œ ì‹¤íŒ¨"}
    stop_event = Event()

    # 1. ì´ˆê¸° ì‘ë‹µ(LLM1)ì™€ LLM2 ë¹Œë“œ ë™ì‹œ ì‹œì‘
    initial_task = create_task(
        run_initial_controller(
            user_query=user_query,
            faiss_db=faiss_db,
            current_yes_count=yes_count,
            template_data=template_data,
            stop_event=stop_event,
        )
    )
    # LLM2 ë¹Œë“œëŠ” LLM1ì˜ ìŠ¤íƒ€íŠ¸ì™€ ë™ì‹œì— ì‹¤í–‰ (build_only=True)
    build_task = None
    if not llm2_lock.locked():
        build_task = create_task(
            run_full_consultation(
                user_query,
                faiss_kiwi.extract_top_keywords_faiss(user_query, faiss_db),
                model="gpt-4",
                build_only=True,  # ì´ˆê¸° ë¹Œë“œëŠ” build_only ëª¨ë“œë¡œ ì‹œì‘
                stop_event=stop_event,
            )
        )
    else:
        print("âš ï¸ LLM2 ë¹Œë“œê°€ ì´ë¯¸ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤.")

    # 2. ì´ˆê¸° ì‘ë‹µ(LLM1) ë¨¼ì € ëŒ€ê¸°
    initial_result = await initial_task
    status = initial_result.get("status", "ok")
    if status in ["no_triggered", "nonlegal_skipped"] or stop_event.is_set():
        # print(f"ğŸ›‘ [ë¹Œë“œ ì¤‘ë‹¨] status={status} ë˜ëŠ” noê°ì§€ â†’ íŒë¡€/ì „ëµ ì¤‘ë‹¨")
        if build_task:
            build_task.cancel()
            try:
                await build_task
            except asyncio.CancelledError:
                print("âœ… build_task ì •ìƒì ìœ¼ë¡œ ì·¨ì†Œë¨.")
        return {"initial": initial_result, "advanced": None}

    # ì—…ë°ì´íŠ¸ëœ YES ì¹´ìš´íŠ¸
    yes_count = initial_result.get("yes_count", yes_count)

    # 3. ì´ˆê¸° ì‘ë‹µì— "###yes" ì‹ í˜¸ê°€ ìˆìœ¼ë©´(LLM1 ì‹ í˜¸) ì´ë¯¸ ì‹œì‘ëœ LLM2 ë¹Œë“œ ê²°ê³¼ë¥¼ ê¸°ë‹¤ë¦¼
    if "###yes" in initial_result.get("initial_response", "").lower():
        # print("â„¹ï¸ LLM1 ì‹ í˜¸ ê°ì§€ë¨. LLM2 ë¹Œë“œ ê²°ê³¼ë¥¼ ê¸°ë‹¤ë¦½ë‹ˆë‹¤.")
        # ë§Œì•½ build_taskì´ ì—†ë‹¤ë©´, ìƒˆë¡œ full buildë¡œ ì‹¤í–‰ (build_only=False)
        if not build_task:
            build_task = create_task(
                run_full_consultation(
                    user_query,
                    faiss_kiwi.extract_top_keywords_faiss(user_query, faiss_db),
                    model="gpt-4",
                    build_only=False,  # full build ëª¨ë“œ
                    stop_event=stop_event,
                )
            )
        # ê·¸ë ‡ì§€ ì•Šìœ¼ë©´ ì´ë¯¸ ì§„í–‰ ì¤‘ì¸ build_taskì˜ ê²°ê³¼ë¥¼ ê·¸ëŒ€ë¡œ ê¸°ë‹¤ë¦½ë‹ˆë‹¤.

    # 4. LLM2 ë¹Œë“œ ê²°ê³¼ ëŒ€ê¸° (ì´ˆê¸° ì‘ë‹µê³¼ ë³„ê°œë¡œ ì§„í–‰)
    prepared_data = {}
    if build_task:
        prepared_data = await build_task

    template, strategy, precedent = (
        prepared_data.get("template"),
        prepared_data.get("strategy"),
        prepared_data.get("precedent"),
    )
    if not all([template, strategy, precedent]):
        print("âš ï¸ ë¹Œë“œ ì‹¤íŒ¨.")
        return {"initial": initial_result, "advanced": None}

    # 5. ê³ ê¸‰ ì‘ë‹µ(LLM2 ìµœì¢… ì‘ë‹µ)ì€ ê¸°ì¡´ ì¡°ê±´(yes_count >= 3) ë§Œì¡± ì‹œì—ë§Œ ìƒì„±ë¨
    advanced_result = None
    if yes_count >= 3:
        async with llm2_lock:
            final_answer = run_final_answer_generation(
                template, strategy, precedent, user_query, "gpt-4"
            )
            yes_count = 1  # YES ì¹´ìš´íŠ¸ ì´ˆê¸°í™”
            advanced_result = {
                "template": template,
                "strategy": strategy,
                "precedent": precedent,
                "final_answer": final_answer,
                "status": "ok",
            }

    return {"initial": initial_result, "advanced": advanced_result}


async def chatbot_loop():
    while True:
        user_query = input("\nâ“ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œ: exit): ")
        if user_query.lower() == "exit":
            break

        if llm2_lock.locked():
            continue

        result = await run_dual_pipeline(user_query)
        if "error" in result:
            continue

        initial = result["initial"]

        # âœ… ë¬´ì¡°ê±´ ì¶œë ¥: mcq_question â†’ fallback to initial_response
        mcq = initial.get("mcq_question") or initial.get("initial_response")
        if mcq:
            print(mcq)
        else:
            print("\nì—†ìŒ")

        advanced = result.get("advanced")
        if advanced and advanced.get("final_answer"):
            print("\nğŸš€ [ê³ ê¸‰ LLM ì‘ë‹µ]:")
            print(
                "ğŸ“„ í…œí”Œë¦¿ ìš”ì•½:", advanced.get("template", {}).get("summary", "ì—†ìŒ")
            )
            print(
                "ğŸ§  ì „ëµ ìš”ì•½:",
                advanced.get("strategy", {}).get("final_strategy_summary", "ì—†ìŒ"),
            )
            print("ğŸ“š íŒë¡€ ìš”ì•½:", advanced.get("precedent", {}).get("summary", "ì—†ìŒ"))
            print("ğŸ”— ë§í¬:", advanced.get("precedent", {}).get("casenote_url", "ì—†ìŒ"))
            print("\nğŸ¤– ìµœì¢… GPT ì‘ë‹µ:\n", advanced.get("final_answer", "ì—†ìŒ"))
        else:
            print("\nâœ… ì´ˆê¸° ì‘ë‹µìœ¼ë¡œ ì¶©ë¶„í•©ë‹ˆë‹¤.")


app.include_router(chatbot_router, prefix="/api/chatbot")

def main():
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)
    loop.run_until_complete(chatbot_loop())
    loop.close()


if __name__ == "__main__":
    main()
