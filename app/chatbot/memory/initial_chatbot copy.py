import os
import sys
from dotenv import load_dotenv
from langchain.memory import ConversationBufferMemory
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI
from asyncio import Event

from app.chatbot.tool_agents.utils.utils import (
    faiss_kiwi,
    classify_legal_query,
)
from app.chatbot.tool_agents.tools import async_ES_search_one

load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")


def load_llm():
    return ChatOpenAI(
        model="gpt-3.5-turbo",
        api_key=OPENAI_API_KEY,
        temperature=0.6,
        max_tokens=256,
        streaming=False,
    )


class LegalChatbot:
    def __init__(self, faiss_db):
        self.llm = load_llm()
        self.memory = ConversationBufferMemory(
            memory_key="chat_history", return_messages=True
        )
        self.faiss_db = faiss_db
        self.prompt_template = PromptTemplate(
            template="""
        ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ì˜ ë²•ë¥  ì „ë¬¸ê°€ì…ë‹ˆë‹¤.  
        í˜„ì¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•œ **ë²•ë¥ ì  íƒ€ë‹¹ì„±, ì •ë³´ì˜ ëª…í™•ì„±, ìœ ì‚¬ ì‚¬ë¡€ì™€ì˜ ì í•©ì„±**ì„ ê¸°ì¤€ìœ¼ë¡œ  
        'ì‹¤ì‹œê°„ ë³´ê³ ì„œ' í˜•íƒœë¡œ í‰ê°€í•´ ì£¼ì„¸ìš”.

        ë‹¤ìŒ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ íŒë‹¨í•˜ì„¸ìš”:

        ğŸ’¬ ëŒ€í™” ê¸°ë¡:
        {chat_history}

        â“ ì‚¬ìš©ì ì§ˆë¬¸:
        "{user_query}"

        ğŸ§  ì‚¬ìš©ì ì…ë ¥ í‚¤ì›Œë“œ:
        {query_keywords}

        ğŸ“š FAISS ìœ ì‚¬ í‚¤ì›Œë“œ:
        {faiss_keywords}

        ğŸ“‚ ì§ˆë¬¸ ìœ í˜•:
        {query_type}

        ğŸ“„ ìœ ì‚¬ ìƒë‹´ ê²€ìƒ‰ ê²°ê³¼ (Elasticsearch ê¸°ë°˜):
        {es_context}

        ğŸ“¢ ì§€ì‹œì‚¬í•­:
        - ì•„ë˜ í˜•ì‹ì— ë”°ë¼ ì‹¤ì‹œê°„ íŒë‹¨ ë³´ê³ ì„œë¥¼ ì‘ì„±í•˜ì„¸ìš”.
        - ë‹¨, ì§€ê¸ˆì€ ìƒì„¸í•œ ë¬¸ì¥ì´ ì£¼ì–´ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤.
        - ë¨¼ì € ì²œì²œíˆ ìƒê°í•´ì„œ ë³´ê³ ì„œ í•­ëª© ì¤‘ 0, 1, 2, 4 ë²ˆ í•­ëª©ì„ ìœ ì¶”í•˜ê¸°ì— ê°€ì¥ ì¤‘ìš”í•œ ë‹¨ì–´ 5ê°œë¡œ ì „ë‹¬í•˜ì„¸ìš”.
        - 0, 1, 2, 4 ë²ˆ í•­ëª© ì˜ˆì‹œ =  ê³„ì•½, í•´ì§€, í†µë³´, ìœ„ì•½ê¸ˆ, ë¬¸ì œ
        - 3ë²ˆì€ ìˆ«ìë¡œë§Œ ë‚˜íƒ€ë‚´ì„¸ìš”
        - ë‹¨ì–´ ê°„ ì—°ê´€ì„±ê³¼ ë²•ë¥ ì  ê°€ëŠ¥ì„±ì„ ë°”íƒ•ìœ¼ë¡œ, ìµœëŒ€í•œ ë…¼ë¦¬ì ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.
        - ì§ˆë¬¸ì´ ë²•ë¥ ì ì´ì§€ ì•Šê±°ë‚˜ ë„ˆë¬´ ëª¨í˜¸í•˜ë©´ ë°˜ë“œì‹œ `###no`ë¡œ ëë‚´ì„¸ìš”.

        -----------------------------

        ğŸ“Œ [ì‹¤ì‹œê°„ íŒë‹¨ ë³´ê³ ì„œ]

        0. ğŸ” ì‚¬ìš©ì ì§ˆë¬¸ ìì²´ì˜ ë²•ë¥ ì„± íŒë‹¨:
        - ì§ˆë¬¸ ìì²´ê°€ ë²•ì ìœ¼ë¡œ ìœ ì˜ë¯¸í•œê°€? ëª…í™•í•˜ê³  êµ¬ì²´ì ì¸ê°€? 5ë‹¨ì–´ í‘œí˜„

        1. ğŸ“ ì œëª©:
        - ë³¸ ì§ˆë¬¸ì— ì í•©í•œ ìƒë‹´ ì œëª©ì„ 5ë‹¨ì–´ë¡œ í‘œí˜„í•˜ì„¸ìš”.

        2. ğŸ‘¥ ì‚¬ìš©ì ìƒí™©ê³¼ ìœ ì‚¬í•œê°€?
        - ìœ ì‚¬í•œ ìƒí™©ì´ ì¡´ì¬í•  ìˆ˜ ìˆë‹¤ê³  íŒë‹¨ë˜ëŠ”ê±¸ 5ë‹¨ì–´ë¡œ í‘œí˜„í•˜ì„¸ìš”

        3. ğŸ“ í‰ê°€ ì ìˆ˜:
        - ì§ˆë¬¸ ëª…í™•ì„± (0~5):
        - ë²•ë¥  ê´€ë ¨ì„± (0~5):
        - ì •ë³´ ì™„ì „ì„± (0~5):
        - ì´ì  (í•©ì‚°):

        4. ğŸ“‹ ì¤‘ê°„ ìš”ì•½:
        - ë²•ë¥ ì ìœ¼ë¡œ ëŒ€ì‘ ê°€ëŠ¥í•œ í•µì‹¬ ë°©í–¥ì„ 5ë‹¨ì–´ë£Œ í‘œí˜„í•˜ì„¸ìš”

        ë§ˆì§€ë§‰ ì¤„ì— íŒë‹¨ ê²°ê³¼ ê¸°ì…:  
        ###yes ë˜ëŠ” ###no
        -----------------------------
        """,
            input_variables=[
                "chat_history",
                "user_query",
                "query_keywords",
                "faiss_keywords",
                "query_type",
                "es_context",
            ],
        )

    async def build_es_context(self, user_query: str) -> str:
        # âœ… ES ê²°ê³¼ ì—†ìœ¼ë©´ ì§ì ‘ ê²€ìƒ‰
        es_results = await async_ES_search_one([user_query])

        if not es_results:
            return "ê´€ë ¨ ìƒë‹´ì‚¬ë¡€ê°€ ì—†ìŠµë‹ˆë‹¤."

        es_context = ""
        for i, item in enumerate(es_results[:1], start=1):  # ìƒìœ„ 3ê°œë§Œ
            es_context += f"\nğŸ“Œ [{i}ë²ˆ ìƒë‹´]\n"
            es_context += f"- ì œëª©(title): {item.get('title', '')}\n"
            es_context += f"- ì§ˆë¬¸(question): {item.get('question', '')}\n"
            es_context += f"- ë‹µë³€(answer): {item.get('answer', '')}\n"
        return es_context.strip()

    async def generate(
        self,
        user_query: str,
        current_yes_count: int = 0,
        stop_event: Event = None,
    ):
        query_keywords = faiss_kiwi.extract_keywords(user_query, top_k=5)
        faiss_keywords = faiss_kiwi.extract_top_keywords_faiss(
            user_query, self.faiss_db, top_k=5
        )
        legal_score = sum(1 for kw in query_keywords if kw in faiss_keywords) / max(
            len(query_keywords), 1
        )
        query_type = classify_legal_query(user_query, set(faiss_keywords))
        chat_history = self.memory.load_memory_variables({}).get("chat_history", "")

        # âœ… ES ìœ ì‚¬ ìƒë‹´ ë‚´ìš© ì¶”ì¶œ
        es_context = await self.build_es_context(user_query)

        # âœ… í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = self.prompt_template.format(
            chat_history=chat_history,
            user_query=user_query,
            query_keywords=", ".join(query_keywords),
            faiss_keywords=", ".join(faiss_keywords),
            legal_score=f"{legal_score:.2f}",
            query_type=query_type,
            es_context=es_context,
        )

        full_response = ""
        is_no_detected = False

        async for chunk in self.llm.astream(prompt):
            content = getattr(chunk, "content", str(chunk))
            if content:
                sys.stdout.write(content)
                sys.stdout.flush()
                full_response += content

                # ì‹¤ì‹œê°„ ê°ì§€
                if "###no" in full_response[-10:].lower():
                    is_no_detected = True
                    if stop_event:
                        stop_event.set()
                    break

        self.memory.save_context(
            {"user_query": user_query}, {"response": full_response}
        )

        return {
            "initial_response": full_response,
            "escalate_to_advanced": False,
            "yes_count": current_yes_count,
            "query_type": query_type,
            "is_no": is_no_detected,
        }
