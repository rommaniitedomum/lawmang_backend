import os
import json
import asyncio
from typing import Optional, Dict, Any
from langchain_openai import ChatOpenAI
from dotenv import load_dotenv
from app.chatbot.tool_agents.tools import LawGoKRTavilySearch, async_ES_search_one
from app.chatbot.tool_agents.utils.utils import (
    insert_hyperlinks_into_text,
    evalandsave_llm2_template_with_es,
    calculate_llm2_accuracy_score,
)
# ê¸€ë¡œë²Œ ìºì‹œ ê¸°ëŠ¥: í…œí”Œë¦¿ì„ ì‹œìŠ¤í…œ ë©”ì‹œì§€ë¡œ ì €ì¥í•˜ê³  ì¡°íšŒí•˜ëŠ” í•¨ìˆ˜ë“¤
from app.chatbot.memory.global_cache import (
    retrieve_template_from_memory,store_template_in_memory
)
from app.chatbot.initial_agents.prompt_tone_selector import get_prompt_by_score
load_dotenv()
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")


def load_llm():
    return ChatOpenAI(
        model="gpt-3.5-turbo",
        api_key=OPENAI_API_KEY,
        temperature=0.3,
        max_tokens=2048,
    )


class AskHumanAgent:
    def __init__(self):
        self.llm = load_llm()
        self.tavily_search = LawGoKRTavilySearch()

    async def build_mcq_prompt_full(
        self,
        user_query,
        llm1_answer,
        template_data,
        yes_count,
        report: Optional[str] = None,
        max_score: Optional[float] = None,
        tone_prompt: Optional[str] = None,
    ):
        template = template_data.get("template", {})
        strategy = template_data.get("strategy", {})
        precedent = template_data.get("precedent", {})

        summary_with_links = insert_hyperlinks_into_text(
            template.get("summary", ""), template.get("hyperlinks", [])
        )
        explanation_with_links = insert_hyperlinks_into_text(
            template.get("explanation", ""), template.get("hyperlinks", [])
        )
        hyperlinks_text = "\n".join(
            f"- {link['label']}: {link['url']}" for link in template.get("hyperlinks", [])
        )
        strategy_decision_tree = "\n".join(strategy.get("decision_tree", []))
        precedent_summary = precedent.get("summary", "íŒë¡€ ìš”ì•½ ì—†ìŒ")
        precedent_link = precedent.get("casenote_url", "ë§í¬ ì—†ìŒ")
        precedent_meta = f"{precedent.get('court', '')} / {precedent.get('j_date', '')} / {precedent.get('title', '')}"

        # âœ… ì •í™•ë„ ê¸°ë°˜ í”„ë¡¬í”„íŠ¸ í†¤ ì¡°ì ˆ
        accuracy = template_data.get("llm2_accuracy_score", 0)
        prompt = get_prompt_by_score(
            max_score=accuracy,  # âœ… ì—¬ê¸° ë°˜ì˜
            user_query=user_query,
            summary_with_links=summary_with_links,
            explanation_with_links=explanation_with_links,
            template=template,
            strategy=strategy,
            strategy_decision_tree=strategy_decision_tree,
            hyperlinks_text=hyperlinks_text,
            precedent_summary=precedent_summary,
            precedent_link=precedent_link,
            precedent_meta=precedent_meta,
        )

        return prompt

    async def generate_mcq_question(
        self,
        user_query,
        llm1_answer,
        yes_count=0,
        template_data=None,
        max_score=0,
        report: Optional[str] = None,
    ):
        prompt = await self.build_mcq_prompt_full(
            user_query,
            llm1_answer,
            template_data or {},
            yes_count,
            max_score,
            report,
        )
        response = await self.llm.ainvoke(prompt)
        return response.content.strip()

    async def ask_human(
        self,
        user_query,
        llm1_answer=None,
        current_yes_count=0,
        template_data=None,
        initial_response: Optional[str] = None,
    ):
        # print("ğŸ” [ask_human] ES prefetch ì‹œì‘")
        es_task = asyncio.create_task(async_ES_search_one([user_query]))

        # print("ğŸ“¦ [ask_human] í…œí”Œë¦¿ ë¡œë”© ì¤‘...")
        cached_data = retrieve_template_from_memory()
        accuracy = 0
        evaluating_now = False

        if cached_data and cached_data.get("built_by_llm2"):
            if not cached_data.get("updated_by_es"):
                # print("ğŸ§  [ask_human] ES ê¸°ë°˜ í‰ê°€ ìˆ˜í–‰ ì¤‘...")
                evaluating_now = True
                await evalandsave_llm2_template_with_es(cached_data, user_query)
                cached_data["updated_by_es"] = True

            template_score = max(
                cached_data.get("template", {}).get("summary_score", 0),
                cached_data.get("template", {}).get("explanation_score", 0),
                cached_data.get("template", {}).get("ref_question_score", 0),
            )
            accuracy = calculate_llm2_accuracy_score(template_score, 0)
            cached_data["llm2_accuracy_score"] = accuracy
            store_template_in_memory(cached_data)
        elif cached_data:
            accuracy = cached_data.get("llm2_accuracy_score", 0)
        else:
            accuracy = 0

        # print("â³ [ask_human] ES ê²°ê³¼ ìˆ˜ì§‘ ëŒ€ê¸°")
        es_result = await es_task
        # print(f"âœ… [ask_human] ES ì™„ë£Œ, max_score={es_result.get('max_score')}")

        max_score = es_result.get("max_score", 0)
        hits = es_result.get("hits", [])

        # âœ… 2. í…œí”Œë¦¿ ë¶ˆëŸ¬ì˜¤ê¸°
        cached_data = retrieve_template_from_memory()
        accuracy = 0
        evaluating_now = False

        # âœ… 3. í‰ê°€ ìˆ˜í–‰ ì¡°ê±´: LLM2 í…œí”Œë¦¿ì´ ì™„ì„±ëœ ìƒíƒœ
        if cached_data and cached_data.get("built_by_llm2"):
            if not cached_data.get("updated_by_es"):
                evaluating_now = True
                await evalandsave_llm2_template_with_es(cached_data, user_query)
                cached_data["updated_by_es"] = True

            # âœ… í‰ê°€ ì ìˆ˜ëŠ” í•­ìƒ ë‹¤ì‹œ ê³„ì‚°
            template_score = max(
                cached_data.get("template", {}).get("summary_score", 0),
                cached_data.get("template", {}).get("explanation_score", 0),
                cached_data.get("template", {}).get("ref_question_score", 0),
            )
            accuracy = calculate_llm2_accuracy_score(template_score, max_score)
            cached_data["llm2_accuracy_score"] = accuracy
            store_template_in_memory(cached_data)

        # âœ… í…œí”Œë¦¿ì´ ì—†ê±°ë‚˜ ì•„ì§ LLM2ê°€ ë¹Œë“œë˜ì§€ ì•Šì€ ê²½ìš°
        elif cached_data:
            accuracy = cached_data.get("llm2_accuracy_score", 0)
        else:
            accuracy = 0


        # âœ… 4. fallback íŒë‹¨ ê¸°ì¤€
        fallback_threshold = 15
        if 0 < accuracy < 50:
            fallback_threshold = int(15 + (50 - accuracy) * 0.5)

        if not evaluating_now and (
            (llm1_answer and "###no" in llm1_answer.lower())
            or (accuracy < 50 and max_score < fallback_threshold)
        ):

            return {
                "yes_count": 0,
                "mcq_question": (
                    "ğŸ’¡ ì´ ì§ˆë¬¸ì€ ë²•ë¥ ì ìœ¼ë¡œ ëª…í™•í•˜ì§€ ì•Šì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
                    "ê°€ëŠ¥í•˜ë‹¤ë©´ ì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ ì„¤ëª…í•´ ì£¼ì‹œê² ì–´ìš”?"
                ),
                "is_mcq": False,
                "load_template_signal": False,
                "template": {},
            }

        # print("âœ… [ask_human] LLM2 í…œí”Œë¦¿ ì‚¬ìš© ì¤‘")
        template_data = cached_data or template_data

        # âœ… 5. default í…œí”Œë¦¿ (fallbackì—ì„œ ìƒì„±ë˜ëŠ” ê²½ìš°)
        if not template_data:
            # print("âš ï¸ [ask_human] default í…œí”Œë¦¿ ìƒì„± ì‹œì‘")
            titles = [item.get("title", "") for item in hits if item.get("title")]
            template_data = {
                "template": {
                    "summary": "ë‹¹ì‹ ì€ ì¹œì ˆí•œ ìƒë‹´ì‚¬ì…ë‹ˆë‹¤.",
                    "explanation": "",
                    "ref_question": "",
                    "hyperlinks": [],
                },
                "strategy": {
                    "final_strategy_summary": "",
                    "tone": "ì¹œì ˆí•˜ê³  ëª…í™•í•œ ë§íˆ¬",
                    "structure": "ë¬¸ì œì œê¸° â†’ ë²•ì ìš”ê±´ â†’ ê²°ë¡ ",
                    "decision_tree": [
                        f"ì‚¬ìš©ìê°€ '{title}' ê´€ë ¨ ì§ˆë¬¸ì„ í•˜ë©´ ë²•ì  ìš”ê±´ìœ¼ë¡œ íŒë‹¨"
                        for title in titles[:3]
                    ],
                },
                "precedent": {},
            }

        # âœ… 6. YES ê°ì§€ ë° ëˆ„ì 
        yes_count_detected = 1 if llm1_answer and "###yes" in llm1_answer.lower() else 0
        total_yes_count = current_yes_count + yes_count_detected

        # âœ… 7. í›„ì† ì§ˆë¬¸ ìƒì„±
        mcq_q = await self.generate_mcq_question(
            user_query, llm1_answer or "", total_yes_count, template_data
        )
        if total_yes_count >= 2:
            mcq_q = f"{mcq_q}\n\n[ì €ì¥ëœ í…œí”Œë¦¿ ì‚¬ìš©ë¨]"

        # âœ… 8. ë³‘í•© ì‘ë‹µ
        combined = (
            f"{initial_response.strip()}\n\nğŸ§© [í›„ì† ì§ˆë¬¸ ì œì•ˆ]\n{mcq_q.strip()}"
            if initial_response
            else mcq_q.strip()
        )

        return {
            "yes_count": total_yes_count,
            "mcq_question": combined,
            "is_mcq": True,
            "load_template_signal": total_yes_count >= 2,
            "template": template_data,
        }
