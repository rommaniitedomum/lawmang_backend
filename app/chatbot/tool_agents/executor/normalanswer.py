import os
import json
import sys
from langchain_openai import ChatOpenAI
from langchain.schema import HumanMessage, SystemMessage
from app.chatbot.tool_agents.tools import LawGoKRTavilySearch
from app.chatbot.tool_agents.utils.utils import (
    insert_hyperlinks_into_text,
)
from langchain.memory import ConversationBufferMemory
from langchain_teddynote import logging

logging.langsmith("llamaproject")

# âœ… LangChain ChatOpenAI
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")

memory = ConversationBufferMemory(
    memory_key="chat_history", max_token_limit=1000, return_messages=True
)


def build_final_answer_prompt(
    template: dict, strategy: dict, precedent: dict, user_query: str
) -> str:
    precedent_summary = precedent.get("summary", "")
    precedent_link = precedent.get("casenote_url", "")
    precedent_meta = f"{precedent.get('court', '')} / {precedent.get('j_date', '')} / {precedent.get('title', '')}"

    summary_with_links = insert_hyperlinks_into_text(
        template["summary"], template.get("hyperlinks", [])
    )
    explanation_with_links = insert_hyperlinks_into_text(
        template["explanation"], template.get("hyperlinks", [])
    )
    hyperlinks_text = "\n".join(
        [f"- {link['label']}: {link['url']}" for link in template.get("hyperlinks", [])]
    )
    strategy_decision_tree = "\n".join(strategy.get("decision_tree", []))

    chat_history = memory.load_memory_variables({}).get("chat_history", "")

    prompt = f"""
ë‹¹ì‹ ì€ ë²•ë¥  ìƒë‹´ì„ ìƒì„±í•˜ëŠ” ê³ ê¸‰ AIì…ë‹ˆë‹¤.

[ëŒ€í™” íˆìŠ¤í† ë¦¬]
{chat_history}

[ì‚¬ìš©ì ì§ˆë¬¸]
{user_query}

[ìš”ì•½]
{summary_with_links}

[ì„¤ëª…]
{explanation_with_links}

[ì°¸ê³  ì§ˆë¬¸]
{template["ref_question"]}

[í•˜ì´í¼ë§í¬]
{hyperlinks_text}

[ì „ëµ ìš”ì•½]
{strategy.get("final_strategy_summary", "")}

[ì‘ë‹µ êµ¬ì„± ì „ëµ]
- ë§íˆ¬: {strategy.get("tone", "")}
- íë¦„: {strategy.get("structure", "")}
- ì¡°ê±´ íë¦„ë„:
{strategy_decision_tree}

[ì¶”ì²œ ë§í¬]
{json.dumps(strategy.get("recommended_links", []), ensure_ascii=False)}

[ì¶”ê°€ëœ íŒë¡€ ìš”ì•½]
- {precedent_summary}
- ë§í¬: {precedent_link}
- ì •ë³´: {precedent_meta}

ğŸ’¡ ìœ„ ë‚´ìš©ì„ ë°˜ì˜í•˜ì—¬, ì‚¬ìš©ìê°€ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë²•ë¥  ìƒë‹´ì„ ìƒì„±í•˜ì„¸ìš”.
"""
    return prompt.strip()


def run_final_answer_generation(
    template: dict,
    strategy: dict,
    precedent: dict,
    user_query: str,
    model: str = "gpt-4",
) -> str:
    final_prompt = build_final_answer_prompt(template, strategy, precedent, user_query)
    print("\nğŸ¤– AI ë‹µë³€:")
    final_answer = ""

    # âœ… LangChain ChatOpenAI (Streaming)
    llm = ChatOpenAI(
        model=model,
        api_key=OPENAI_API_KEY,
        temperature=0.4,
        streaming=True
    )

    messages = [
        SystemMessage(
            content="ë‹¹ì‹ ì€ ê³ ê¸‰ ë²•ë¥  ì‘ë‹µì„ ìƒì„±í•˜ëŠ” AIì…ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ì‹ ë¢°ë¥¼ ì–»ì„ ìˆ˜ ìˆëŠ” ì •í™•í•˜ê³  ìì—°ìŠ¤ëŸ¬ìš´ ìƒë‹´ì„ ìƒì„±í•˜ì„¸ìš”."
        ),
        HumanMessage(content=final_prompt),
    ]

    # âœ… ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì²˜ë¦¬
    for chunk in llm.stream(messages):
        if hasattr(chunk, "content") and chunk.content:
            sys.stdout.write(chunk.content)
            sys.stdout.flush()
            final_answer += chunk.content

    # âœ… ë©”ëª¨ë¦¬ì— ì €ì¥
    memory.save_context(
        {"user_query": user_query}, {"response": precedent.get("summary", "")}
    )

    return final_answer
