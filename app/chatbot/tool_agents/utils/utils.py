import re
from kiwipiepy import Kiwi
from typing import List, Set, Dict
from collections import Counter
import asyncio
from app.chatbot.memory.global_cache import store_template_in_memory
from app.chatbot.tool_agents.tools import async_ES_search_one

kiwi = Kiwi()


def insert_hyperlinks_into_text(text: str, hyperlinks: list) -> str:
    if not hyperlinks:
        return text

    for link in hyperlinks:
        label = link.get("label")
        url = link.get("url")
        tooltip = link.get("tooltip", "")

        if not label or not url:
            continue

        # ğŸ” ì •ê·œì‹ìœ¼ë¡œ ë‹¨ì–´ ê²½ê³„ë§Œ ë§¤ì¹­, ì²« ë²ˆì§¸ í•­ëª©ë§Œ êµì²´
        pattern = r"\b" + re.escape(label) + r"\b"
        hyperlink_html = f'<a href="{url}" title="{tooltip}">{label}</a>'
        text = re.sub(pattern, hyperlink_html, text, count=1)

    return text


# --------------------------------------------------------------------------------


def extract_json_from_text(text):
    match = re.search(r"\{[\s\S]*\}", text)
    if match:
        return match.group(0)
    return None


# --------------------------------------------------------------------------------
# ë²•ë¥  ì—¬ë¶€ íŒë‹¨ ê¸°ì¤€: FAISS í‚¤ì›Œë“œ ê¸°ë°˜
def is_legal_query(keywords: List[str], legal_terms: Set[str], threshold=0.3) -> bool:
    legal_count = sum(1 for kw in keywords if kw in legal_terms)
    ratio = legal_count / len(keywords) if keywords else 0
    return ratio >= threshold


def classify_legal_query(user_input: str, legal_terms: Set[str]) -> str:
    tokens = kiwi.tokenize(user_input)
    extracted = [token.form for token in tokens if token.tag in ("NNG", "NNP")]

    if not extracted:
        return "nonlegal"

    legal_count = sum(1 for word in extracted if word in legal_terms)
    ratio = legal_count / len(extracted)

    return "legal" if ratio >= 0.3 else "nonlegal"


# --------------------------------------------------------------------------------
class faiss_kiwi:
    @staticmethod
    def jaccard_similarity(set1, set2):
        """Jaccard ìœ ì‚¬ë„ë¥¼ ì´ìš©í•œ í‚¤ì›Œë“œ ë¹„êµ"""
        intersection = set1.intersection(set2)
        union = set1.union(set2)
        return len(intersection) / len(union) if len(union) > 0 else 0

    @staticmethod
    def extract_top_keywords(es_result, top_k=5):
        if isinstance(es_result, dict) and "hits" in es_result:
            blocks = es_result["hits"]
        elif isinstance(es_result, list):
            blocks = es_result
        else:
            # print("âš ï¸ [extract_top_keywords] ë¹„ì •ìƒì ì¸ es_result:", es_result)
            return []

        return extract_top_keywords(blocks, top_k=top_k)

    @staticmethod
    def filter_keywords_with_jaccard(user_keywords, faiss_keywords, threshold=0.15):
        """ìì¹´ë“œ ìœ ì‚¬ë„ë¥¼ í™œìš©í•˜ì—¬ FAISS í‚¤ì›Œë“œë¥¼ í•„í„°ë§ (ìœ ì € í‚¤ì›Œë“œ ìœ ì§€)"""
        filtered_keywords = set(user_keywords)  # âœ… ìœ ì € ì…ë ¥ í‚¤ì›Œë“œë¥¼ ë¬´ì¡°ê±´ í¬í•¨

        for faiss_word in faiss_keywords:
            max_sim = max(
                faiss_kiwi.jaccard_similarity(set(faiss_word), set(user_word))
                for user_word in user_keywords
            )
            if max_sim >= threshold:
                filtered_keywords.add(faiss_word)

        return list(filtered_keywords)

    @staticmethod
    def extract_keywords(text: str, top_k: int = 5) -> list[str]:
        """
        ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ì—ì„œ ëª…ì‚¬(NNG, NNP) ê¸°ë°˜ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        """
        tokens = kiwi.tokenize(text)
        nouns = [token.form for token in tokens if token.tag in ("NNG", "NNP")]
        return nouns[:top_k]

    @staticmethod
    def adjust_faiss_keywords(user_input, faiss_keywords):
        """ìœ ì € ì…ë ¥ í‚¤ì›Œë“œì™€ FAISS í‚¤ì›Œë“œë¥¼ ëª¨ë‘ í¬í•¨í•˜ì—¬ ê²€ìƒ‰"""
        user_keywords = faiss_kiwi.extract_keywords(user_input, top_k=5)
        adjusted_keywords = list(set(user_keywords + faiss_keywords))

        # print(f"âœ… [ìµœì¢… ê²€ìƒ‰ í‚¤ì›Œë“œ]: {adjusted_keywords}")
        return adjusted_keywords

    @staticmethod
    def extract_top_keywords_faiss(user_input, faiss_db, top_k=5):
        """FAISS ê²€ìƒ‰ í›„ ìƒìœ„ í‚¤ì›Œë“œ ì¶”ì¶œ (ìœ ì € ì…ë ¥ ë°˜ì˜)"""
        # print(f"ğŸ” [FAISS í‚¤ì›Œë“œ ì¶”ì¶œ] ì…ë ¥: {user_input}")

        search_results = faiss_db.similarity_search(user_input, k=15)
        all_text = " ".join([doc.page_content for doc in search_results])

        faiss_keywords = faiss_kiwi.extract_keywords(all_text, top_k)
        adjusted_keywords = faiss_kiwi.adjust_faiss_keywords(user_input, faiss_keywords)

        # print(f"âœ… [FAISS ìµœì¢… ê²€ìƒ‰ í‚¤ì›Œë“œ] {adjusted_keywords}")
        return adjusted_keywords


def validate_model_type(model):
    if not isinstance(model, str):
        raise TypeError(
            f"âŒ model ì¸ìëŠ” ë¬¸ìì—´(str)ì´ì–´ì•¼ í•©ë‹ˆë‹¤. í˜„ì¬: {type(model)}, ê°’: {model}"
        )


def extract_top_keywords(text_blocks: list[dict], top_k=5) -> list[str]:
    """
    Elasticsearch ê²°ê³¼ ì¤‘ question/answerì—ì„œ ê°€ì¥ ìì£¼ ë“±ì¥í•˜ëŠ” ëª…ì‚¬ ê¸°ë°˜ ìƒìœ„ í‚¤ì›Œë“œ ì¶”ì¶œ
    """
    all_text = " ".join(
        block.get("question", "") + " " + block.get("answer", "")
        for block in text_blocks
    )

    # í•œê¸€/ì˜ë¬¸ ë‹¨ì–´ ì¶”ì¶œ
    tokens = re.findall(r"[ê°€-í£a-zA-Z]{2,}", all_text)

    # ìƒìœ„ ë¹ˆë„ ë‹¨ì–´ ì¶”ì¶œ
    counter = Counter(tokens)
    top_keywords = [word for word, _ in counter.most_common(top_k)]

    return top_keywords


# ------------------------------------------------------------------


async def update_llm2_template_with_es(template_data: Dict, user_query: str) -> None:
    template = template_data.get("template", {}) or {}
    strategy = template_data.get("strategy", {}) or {}
    precedent = template_data.get("precedent", {}) or {}

    async def get_keywords_from_texts(texts, top_k=5):
        es_result = await async_ES_search_one(texts)
        hits = es_result.get("hits", [])
        return extract_top_keywords(hits, top_k=top_k) or ["ê¸°ë³¸"]

    # 1. Summaryìš© í‚¤ì›Œë“œ
    summary_keywords = await get_keywords_from_texts(
        [user_query, template.get("summary", "")]
    )
    template["summary"] = "updated.summary.from.es: " + ", ".join(summary_keywords)

    # 2. Explanation
    explanation_keywords = await get_keywords_from_texts(
        [template.get("explanation", ""), strategy.get("final_strategy_summary", "")]
    )
    template["explanation"] = "updated.explanation.from.es: " + ", ".join(
        explanation_keywords
    )

    # 3. Ref question
    ref_keywords = await get_keywords_from_texts([template.get("ref_question", "")])
    template["ref_question"] = "updated.ref_question.from.es: " + ", ".join(
        ref_keywords
    )

    # 4. Strategy summary
    strat_keywords = await get_keywords_from_texts(
        [
            strategy.get("final_strategy_summary", ""),
            " ".join(strategy.get("decision_tree", [])),
        ]
    )
    strategy["final_strategy_summary"] = "updated.strategy.from.es: " + ", ".join(
        strat_keywords
    )
    strategy["decision_tree"] = [
        f"ì‚¬ìš©ìê°€ '{kw}' ê´€ë ¨ ì§ˆë¬¸ì„ í•˜ë©´ ë²•ì  ìš”ê±´ì„ ì¤‘ì‹¬ìœ¼ë¡œ íŒë‹¨"
        for kw in strat_keywords
    ]

    # 5. Precedent
    prec_keywords = await get_keywords_from_texts(
        [precedent.get("summary", ""), precedent.get("title", "")]
    )
    precedent["summary"] = "updated.precedent.from.es: " + ", ".join(prec_keywords)
    precedent["title"] = f"{prec_keywords[0]} ê´€ë ¨ ì¦ê°• íŒë¡€"

    store_template_in_memory(
        {
            "built": True,
            "built_by_llm2": True,
            "updated_by_es": True,
            "template": template,
            "strategy": strategy,
            "precedent": precedent,
        }
    )


async def evalandsave_llm2_template_with_es(
    template_data: Dict, user_query: str
) -> None:
    template = template_data.get("template", {}) or {}
    strategy = template_data.get("strategy", {}) or {}
    precedent = template_data.get("precedent", {}) or {}

    async def get_keywords_and_score(texts, top_k=5):
        es_result = await async_ES_search_one(texts)
        hits = es_result.get("hits", [])
        max_score = es_result.get("max_score", 0)
        return extract_top_keywords(hits, top_k=top_k) or ["ê¸°ë³¸"], max_score

    # âœ… 1. Summaryìš© í‚¤ì›Œë“œ + ì ìˆ˜
    summary_keywords, summary_score = await get_keywords_and_score(
        [user_query, template.get("summary", "")]
    )
    updated_summary = "updated.summary.from.es: " + ", ".join(summary_keywords)
    template["summary"] = updated_summary
    template["summary_raw"] = template.get("summary", "")  # ì›ë¬¸ ë³´ì¡´
    template["summary_score"] = summary_score

    # âœ… 2. Explanation
    explanation_keywords, explanation_score = await get_keywords_and_score(
        [template.get("explanation", ""), strategy.get("final_strategy_summary", "")]
    )
    updated_explanation = "updated.explanation.from.es: " + ", ".join(
        explanation_keywords
    )
    template["explanation"] = updated_explanation
    template["explanation_raw"] = template.get("explanation", "")
    template["explanation_score"] = explanation_score

    # âœ… 3. Ref question
    ref_keywords, ref_score = await get_keywords_and_score(
        [template.get("ref_question", "")]
    )
    updated_ref_question = "updated.ref_question.from.es: " + ", ".join(ref_keywords)
    template["ref_question"] = updated_ref_question
    template["ref_question_raw"] = template.get("ref_question", "")
    template["ref_question_score"] = ref_score

    # âœ… 4. Strategy summary
    strat_keywords, _ = await get_keywords_and_score(
        [
            strategy.get("final_strategy_summary", ""),
            " ".join(strategy.get("decision_tree", [])),
        ]
    )
    strategy["final_strategy_summary"] = "updated.strategy.from.es: " + ", ".join(
        strat_keywords
    )
    strategy["decision_tree"] = [
        f"ì‚¬ìš©ìê°€ '{kw}' ê´€ë ¨ ì§ˆë¬¸ì„ í•˜ë©´ ë²•ì  ìš”ê±´ì„ ì¤‘ì‹¬ìœ¼ë¡œ íŒë‹¨"
        for kw in strat_keywords
    ]

    # âœ… 5. Precedent
    prec_keywords, _ = await get_keywords_and_score(
        [precedent.get("summary", ""), precedent.get("title", "")]
    )
    precedent["summary"] = "updated.precedent.from.es: " + ", ".join(prec_keywords)
    precedent["title"] = f"{prec_keywords[0]} ê´€ë ¨ ì¦ê°• íŒë¡€"

    # âœ… ìºì‹œ ì €ì¥
    store_template_in_memory(
        {
            "built": True,
            "built_by_llm2": True,
            "updated_by_es": True,
            "template": template,
            "strategy": strategy,
            "precedent": precedent,
        }
    )


def calculate_llm2_accuracy_score(template_score: float, user_score: float) -> int:
    """
    ì‚¬ìš©ì ì ìˆ˜(user_score)ëŠ” ë³´ì¡° ì‹ í˜¸ë¡œë§Œ ì‚¬ìš©í•˜ê³ ,
    template_scoreë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ 10~100 ìŠ¤ì¼€ì¼ì˜ ì •í™•ë„ë¥¼ ê³„ì‚°.
    """
    if not template_score or not user_score:
        return 0

    # 1. í…œí”Œë¦¿ ì ìˆ˜ ì •ê·œí™” (10~100 ì‚¬ì´ ê°’ ê¸°ì¤€)
    template_scaled = min(max(template_score, 10), 100)

    # 2. ì‚¬ìš©ì ì ìˆ˜ ê°€ì¤‘ì¹˜ ì ìš© (10~25 ë²”ìœ„ â†’ 0.0 ~ 1.0)
    user_weight = (user_score - 10) / 15  # â†’ 0.0 ~ 1.0

    # 3. ì •í™•ë„ ê³„ì‚° (template ì¤‘ì‹¬, userëŠ” ìµœëŒ€ 10ì  ê°€ì‚°)
    final_score = template_scaled + (10 * user_weight)

    return min(100, int(final_score))
