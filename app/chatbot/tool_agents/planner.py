import os
import json
import difflib
from app.chatbot.tool_agents.tools import async_ES_search
from langchain_openai import ChatOpenAI
from typing import List, Dict
from app.chatbot.tool_agents.tools import LawGoKRTavilySearch
from app.chatbot.memory.templates import get_default_strategy_template
from app.chatbot.tool_agents.utils.utils import validate_model_type
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")


def get_llm(model: str, temperature: float = 0.3) -> ChatOpenAI:
    validate_model_type(model)  # âœ… íƒ€ì… ì²´í¬

    return ChatOpenAI(
        model=model,
        api_key=OPENAI_API_KEY,
        temperature=temperature,
        streaming=False,
    )


# âœ… ì‘ë‹µ í…œí”Œë¦¿ ìƒì„±
async def generate_response_template(
    title: str,
    question: str,
    answer: str,
    user_query: str,
    es_results: list[dict] = None,
    model: str = "gpt-3.5-turbo",
) -> dict:
    # âœ… es_resultsê°€ ì—†ìœ¼ë©´ ì§ì ‘ í˜¸ì¶œ
    if es_results is None:
        es_results = await async_ES_search([user_query])

    # ğŸ”¹ ES ìƒë‹´ ë‚´ìš© ì¶”ê°€ êµ¬ì„±
    es_context = ""
    if es_results:
        es_context += "ESì—ì„œ ê²€ìƒ‰í•œ ìœ ì‚¬ ìƒë‹´ 3ê±´:\n"
        for i, item in enumerate(es_results, start=1):
            es_context += f"\nğŸ“Œ [{i}ë²ˆ ìƒë‹´]\n"
            es_context += f"- ì œëª©(title): {item.get('title', '')}\n"
            es_context += f"- ì§ˆë¬¸(question): {item.get('question', '')}\n"
            es_context += f"- ë‹µë³€(answer): {item.get('answer', '')}\n"

    prompt = f"""
ë‹¹ì‹ ì€ ë²•ë¥  ìƒë‹´ ì‘ë‹µ í…œí”Œë¦¿ì„ êµ¬ì„±í•˜ëŠ” AIì…ë‹ˆë‹¤.

ì‚¬ìš©ìì˜ ì§ˆë¬¸:
"{user_query}"

{es_context}

ì„ íƒëœ ëŒ€í‘œ ìƒë‹´(title):
"{title}"

ìƒë‹´ ì§ˆë¬¸(question):
"{question}"

ìƒë‹´ ë‹µë³€(answer):
"{answer}"

--- ì‘ì—… ì§€ì‹œ ---
ğŸ’¡ ì œê³µëœ ë‹µì¤‘ì—ëŠ” ì˜¤ë‹µì´ ì„ì—¬ ìˆìŠµë‹ˆë‹¤ ì²œì²œíˆ ìƒê°í•´ë³´ê³  ì‚¬ìš©ì ì…ì¥ì—ì„œ ì˜¬ë°”ë¥¸ ë‹µë³€ì„ í•´ë³´ì„¸ìš”.
1. ì‚¬ìš©ìê°€ ì´í•´í•˜ê¸° ì‰½ê²Œ í•µì‹¬ ë‚´ìš©ì„ ìš”ì•½í•˜ì„¸ìš” (summary).
2. ìš”ì•½ì„ ë°”íƒ•ìœ¼ë¡œ, ìƒë‹´ ë‹µë³€ì˜ ë‚´ìš©ì„ ì¼ë°˜ì¸ì´ ì´í•´í•  ìˆ˜ ìˆë„ë¡ í’€ì–´ì„œ ì„¤ëª…í•˜ì„¸ìš” (explanation).
3. ë‹µë³€ê³¼ ê´€ë ¨ëœ ë²•ë ¹/íŒë¡€ê°€ ìˆë‹¤ë©´ í•˜ì´í¼ë§í¬ í˜•íƒœë¡œ ì œê³µí•˜ì„¸ìš”. labelê³¼ urlì„ í¬í•¨í•œ ë¦¬ìŠ¤íŠ¸ í˜•ì‹ (hyperlinks).
4. ê·¸ë¦¬ê³  ì´ ìƒë‹´ì—ì„œ ì‚¬ìš©ëœ `question`ì€ ì°¸ê³ ìš© ì§ˆë¬¸ì´ë¯€ë¡œ 'ref_question'ì´ë¼ëŠ” keyë¡œ ë°˜í™˜í•˜ì„¸ìš”.

--- ì‘ë‹µ ì˜ˆì‹œ ---
{{
  "summary": "...",
  "explanation": "...",
  "hyperlinks": [{{"label": "...", "url": "..."}}],
  "ref_question": "..."
}}
"""

    llm = get_llm(model, temperature=0.3)

    messages = [
        {
            "role": "system",
            "content": "ë‹¹ì‹ ì€ ë²•ë¥  ì‘ë‹µ í…œí”Œë¦¿ì„ ìƒì„±í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.",
        },
        {"role": "user", "content": prompt},
    ]

    try:
        response = llm.invoke(messages)
        return json.loads(response.content)
    except Exception:
        return {"error": "GPT ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨"}


# âœ… ì „ëµ ìƒì„±
async def generate_response_strategy(
    explanation: str,
    user_query: str,
    hyperlinks: list = None,
    previous_strategy: dict = None,
    model: str = "gpt-3.5-turbo",
) -> dict:
    hyperlinks = hyperlinks or []

    hyperlink_text = (
        "\n".join([f"- {item['label']}: {item['url']}" for item in hyperlinks])
        if hyperlinks
        else "ì—†ìŒ"
    )

    previous_strategy_text = (
        json.dumps(previous_strategy, ensure_ascii=False, indent=2)
        if previous_strategy
        else "ì—†ìŒ"
    )

    prompt = f"""
ë‹¹ì‹ ì€ ë²•ë¥  ì‘ë‹µ ì „ëµì„ ì„¤ê³„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

[ì‚¬ìš©ì ì§ˆë¬¸]
"{user_query}"

[ì„¤ëª… ì´ˆì•ˆ]
"{explanation}"

[ê´€ë ¨ ë²•ë¥  ë§í¬]
{hyperlink_text}

[ì´ì „ ì „ëµì´ ìˆëŠ” ê²½ìš° ì°¸ê³ ìš©]
{previous_strategy_text}

--- ì‘ì—… ì§€ì‹œ ---
ğŸ’¡ ì œê³µëœ ë‹µì¤‘ì—ëŠ” ì˜¤ë‹µì´ ì„ì—¬ ìˆìŠµë‹ˆë‹¤ ì²œì²œíˆ ìƒê°í•´ë³´ê³  ì‚¬ìš©ì ì…ì¥ì—ì„œ ì˜¬ë°”ë¥¸ ë‹µë³€ì„ í•´ë³´ì„¸ìš”.
ğŸ’¡ ì œê³µëœ ë‹µë³€ì— ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ì£¼ì œê°€ ëª…ë°±íˆ ë‹¤ë¥¸ê²ƒì´ ì¡´ì¬í•˜ë©´ ì‚­ì œí•´ë²„ë¦¬ì„¸ìš”
ex: ê°€ì¡±ì´ ìƒí•´ë¥¼ ë‹¹í–ˆìŠµë‹ˆë‹¤ -> ì´í˜¼ê´€ë ¨ íŒë¡€ 

1. ì´ì „ ì „ëµì´ ìˆë‹¤ë©´ ìµœëŒ€í•œ í™œìš©í•˜ì—¬ ë³´ì™„ëœ ì „ëµì„ ì„¤ê³„í•˜ì„¸ìš”.
2. ì‚¬ìš©ì ê²½í—˜ì„ ê³ ë ¤í•´ ì ì ˆí•œ ë§íˆ¬(tone/style)ë¥¼ ì„¤ê³„í•˜ì„¸ìš”.  
3. ì‘ë‹µ íë¦„ êµ¬ì¡°ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.  
4. ì¡°ê±´/ì˜ˆì™¸ íë¦„ì´ ìˆë‹¤ë©´ decision_tree í˜•ì‹ìœ¼ë¡œ ë§Œë“œì„¸ìš”.  
5. ì „ì²´ ì „ëµì„ ìš”ì•½í•˜ì„¸ìš”.  
6. ì¶”ì²œ ë§í¬ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ì •ë¦¬í•˜ì„¸ìš”.

ì‘ë‹µ í˜•ì‹ (JSON):
{{
  "tone": "...",
  "structure": "...",
  "decision_tree": ["..."],
  "final_strategy_summary": "...",
  "recommended_links": [{{"label": "...", "url": "..."}}]
}}
"""

    llm = get_llm(model, temperature=0.3)

    messages = [
        {"role": "system", "content": "ë‹¹ì‹ ì€ ë²•ë¥  ìƒë‹´ ì „ëµì„ ì„¤ê³„í•˜ëŠ” AIì…ë‹ˆë‹¤."},
        {"role": "user", "content": prompt},
    ]

    try:
        response = llm.invoke(messages)
        strategy_raw = response.content
        strategy = json.loads(strategy_raw)
    except Exception as e:
        default_strategy = get_default_strategy_template()
        default_strategy["error"] = "GPT ì „ëµ íŒŒì‹± ì‹¤íŒ¨"
        return default_strategy

    search_tool = LawGoKRTavilySearch(max_results=3)
    tavily_results = search_tool.run(user_query)

    evaluation = await evaluate_strategy_with_tavily(strategy, tavily_results)
    strategy["evaluation"] = evaluation

    return strategy


# âœ… ì „ëµ í‰ê°€
async def evaluate_strategy_with_tavily(
    strategy: dict,
    tavily_results: list,
    model: str = "gpt-3.5-turbo",
) -> dict:
    if not tavily_results or not isinstance(tavily_results, list):
        return {
            "needs_revision": False,
            "reason": "Tavily ê²°ê³¼ê°€ ì—†ê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•ŠìŒ",
            "tavily_snippets": [],
        }

    tavily_snippets = [
        (result.get("content") or result.get("snippet") or result.get("text")).strip()
        for result in tavily_results[:3]
        if result.get("content") or result.get("snippet") or result.get("text")
    ]

    if not tavily_snippets:
        return {
            "needs_revision": False,
            "reason": "Tavily ìš”ì•½ ì¶”ì¶œ ì‹¤íŒ¨",
            "tavily_snippets": [],
        }

    combined = "\n\n".join(
        [f"[ìš”ì•½ {i + 1}]\n{text}" for i, text in enumerate(tavily_snippets)]
    )

    prompt = f"""
ë‹¹ì‹ ì€ ë²•ë¥  ìƒë‹´ ì „ëµì„ í‰ê°€í•˜ëŠ” AIì…ë‹ˆë‹¤.

[GPT ì „ëµ ìš”ì•½]
{strategy.get("final_strategy_summary", "")}

[Tavily ìš”ì•½ ê²°ê³¼ë“¤]
{combined}

--- ì‘ì—… ì§€ì‹œ ---
GPT ì „ëµì´ ë¶€ì‹¤í•˜ê±°ë‚˜ ì¤‘ìš”í•œ ì •ë³´ë¥¼ ëˆ„ë½í–ˆëŠ”ì§€ í‰ê°€í•˜ì„¸ìš”.
ì•„ë˜ JSONìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.

{{
  "needs_revision": true or false,
  "reason": "...",
  "tavily_snippets": [...]
}}
"""

    llm = get_llm(model, temperature=0.2)
    messages = [
        {"role": "system", "content": "ë²•ë¥  ë¶„ì„ê°€ AIì…ë‹ˆë‹¤."},
        {"role": "user", "content": prompt},
    ]

    try:
        response = llm.invoke(messages)
        return json.loads(response.content)
    except Exception as e:
        return {
            "needs_revision": False,
            "reason": "GPT ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨",
            "tavily_snippets": tavily_snippets,
        }


# âœ… ì „ëµ ë³´ì™„
async def revise_strategy_with_feedback(
    original_strategy: dict,
    tavily_snippets: list,
    model: str = "gpt-3.5-turbo",
) -> dict:
    combined_snippets = "\n\n".join(
        [
            f"[Tavily ìš”ì•½ {i + 1}]\n{snippet}"
            for i, snippet in enumerate(tavily_snippets)
        ]
    )

    prompt = f"""
GPTê°€ ë§Œë“  ê¸°ì¡´ ì „ëµì´ ë„ˆë¬´ ëª¨í˜¸í•˜ê±°ë‚˜ í•µì‹¬ ì •ë³´ë¥¼ ëˆ„ë½í•œ ê²ƒìœ¼ë¡œ íŒë‹¨ë©ë‹ˆë‹¤.  
ì•„ë˜ Tavily ìš”ì•½ì„ ì°¸ê³ í•˜ì—¬ ì „ëµì„ ë³´ì™„í•˜ì„¸ìš”.
[Tavily ìš”ì•½ë“¤]
{combined_snippets}

[ê¸°ì¡´ ì „ëµ ìš”ì•½]
{original_strategy.get("final_strategy_summary", "")}

--- ì‘ì—… ì§€ì‹œ ---
- ê¸°ì¡´ ì „ëµì„ ê¸°ë°˜ìœ¼ë¡œ í•˜ë˜, Tavilyì˜ ë²•ë ¹ ìš”ì•½ì„ ë°˜ì˜í•˜ì—¬ ë” ëª…í™•í•˜ê²Œ ìˆ˜ì •í•˜ì„¸ìš”.
- ì „ì²´ ì „ëµ JSON êµ¬ì¡°ëŠ” ìœ ì§€í•˜ì„¸ìš”.

ì‘ë‹µ í˜•ì‹ (JSON):{{
  "tone": "...",
  "structure": "...",
  "decision_tree": ["..."],
  "final_strategy_summary": "...",
  "recommended_links": [{{"label": "...", "url": "..."}}]
}}
"""

    llm = get_llm(model, temperature=0.2)
    messages = [
        {"role": "system", "content": "ë‹¹ì‹ ì€ ì „ëµ ë³´ì™„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."},
        {"role": "user", "content": prompt},
    ]

    try:
        response = llm.invoke(messages)
        return json.loads(response.content)
    except Exception as e:
        return get_default_strategy_template()


# âœ… ì „ëµ ì‹¤í–‰ íë¦„
async def run_response_strategy_with_limit(
    explanation,
    user_query,
    hyperlinks,
    model="gpt-3.5-turbo",
    previous_strategy: dict = None,  # âœ… ì¶”ê°€
):
    strategy = await generate_response_strategy(
        explanation=explanation,
        user_query=user_query,
        hyperlinks=hyperlinks,
        previous_strategy=previous_strategy,  # âœ… ì „ë‹¬
        model=model,
    )

    if strategy.get("evaluation", {}).get("needs_revision") is True:
        revised = await revise_strategy_with_feedback(
            original_strategy=strategy,
            tavily_snippets=strategy["evaluation"].get("tavily_snippets", []),
        )
        revised["evaluation"] = {"revised": True}
        return revised

    return strategy
