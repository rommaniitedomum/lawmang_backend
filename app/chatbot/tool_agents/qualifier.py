import os
import json
from typing import List, Dict
from langchain_openai import ChatOpenAI
from app.chatbot.tool_agents.tools import async_search_consultation
from app.chatbot.tool_agents.utils.utils import validate_model_type
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")


def build_relevance_prompt(user_query: str, consultation_results: List[Dict]) -> str:
    formatted = "\n\n".join(
        [
            f"{idx + 1}. ì œëª©: {item['title']}\nì§ˆë¬¸: {item['question']}"
            for idx, item in enumerate(consultation_results)
        ]
    )
    return f"""
ë‹¹ì‹ ì€ ë²•ë¥  ìƒë‹´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì‚¬ìš©ìì˜ ì§ˆë¬¸ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
\"{user_query}\"

ì•„ë˜ëŠ” ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ìˆì„ ìˆ˜ ìˆëŠ” ê¸°ì¡´ ìƒë‹´ë“¤ì…ë‹ˆë‹¤.

ê° í•­ëª©ì€ 'ì œëª©', 'ì§ˆë¬¸'ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.  
â†’ ë§Œì•½ ì•„ë˜ ìƒë‹´ë“¤ì´ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ **ì£¼ì œì ìœ¼ë¡œ ì™„ì „íˆ ë¬´ê´€**í•˜ë‹¤ë©´, "irrelevant"ë¼ê³ ë§Œ ì‘ë‹µí•˜ì„¸ìš”.  
â†’ ì¼ë¶€ë¼ë„ ê´€ë ¨ì´ ìˆë‹¤ë©´, "relevant"ë¼ê³ ë§Œ ì‘ë‹µí•˜ì„¸ìš”.

===== ìƒë‹´ ëª©ë¡ =====
{formatted}
""".strip()


async def check_relevance_to_consultations(
    user_query: str,
    consultation_results: List[Dict],
    model: str = "gpt-3.5-turbo",
) -> bool:
    validate_model_type(model)
    if not consultation_results:
        return False

    prompt = build_relevance_prompt(user_query, consultation_results)

    llm = ChatOpenAI(
        model=model,
        api_key=OPENAI_API_KEY,
        temperature=0.0,
        streaming=False,
    )

    messages = [
        {
            "role": "system",
            "content": "ë‹¹ì‹ ì€ ë²•ë¥  ìƒë‹´ ì§ˆë¬¸ì˜ ì£¼ì œ ê´€ë ¨ì„±ì„ íŒë³„í•˜ëŠ” AIì…ë‹ˆë‹¤.",
        },
        {"role": "user", "content": prompt},
    ]

    response = await llm.ainvoke(messages)
    result_text = response.content.strip().lower()
    return result_text == "relevant"


def build_choose_one_prompt(user_query: str, consultation_results: List[Dict]) -> str:
    formatted = "\n\n".join(
        [
            f"{idx + 1}. ì œëª©: {item['title']}\nì§ˆë¬¸: {item['question']}\në‹µë³€: {item['answer']}"
            for idx, item in enumerate(consultation_results)
        ]
    )
    return f"""
ë‹¹ì‹ ì€ ë²•ë¥  ìƒë‹´ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì‚¬ìš©ìì˜ ì§ˆë¬¸ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
\"{user_query}\"

ì•„ë˜ëŠ” ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ì´ ìˆì„ ìˆ˜ ìˆëŠ” ìƒë‹´ ë°ì´í„° ëª©ë¡ì…ë‹ˆë‹¤.  
ê° í•­ëª©ì€ ì œëª©, ì§ˆë¬¸, ë‹µë³€ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.
ğŸ’¡ ì œê³µëœ ë‹µì¤‘ì—ëŠ” ì˜¤ë‹µì´ ì„ì—¬ ìˆìŠµë‹ˆë‹¤ ì²œì²œíˆ ìƒê°í•´ë³´ê³  ì‚¬ìš©ì ì…ì¥ì—ì„œ ì˜¬ë°”ë¥¸ ë‹µë³€ì„ í•´ë³´ì„¸ìš”.
â›” **ì£¼ì˜:** ì§ˆë¬¸/ë‹µë³€ì´ ì‚¬ìš©ì ì§ˆë¬¸ì˜ ì£¼ì œì™€ ì–´ê¸‹ë‚œ í•­ëª©ì€ ì„ íƒí•˜ì§€ ë§ˆì„¸ìš”.  
âœ… ì‚¬ìš©ì ì§ˆë¬¸ì— ê°€ì¥ ì ì ˆí•˜ê²Œ ë‹µí•  ìˆ˜ ìˆëŠ” ìƒë‹´ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì„¸ìš”.

â†’ **ë°˜ë“œì‹œ ì•„ë˜ í˜•ì‹ì˜ JSON ë°°ì—´ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.**  
ì„ íƒí•  í•­ëª©ì˜ ë²ˆí˜¸ë§Œ ë°˜í™˜í•´ì•¼ í•˜ë©°, ë¦¬ìŠ¤íŠ¸ í˜•íƒœë¡œ ì œê³µí•˜ì„¸ìš”.  

ì˜ˆì‹œ:  
[2]

â†’ ê´€ë ¨ëœ í•­ëª©ì´ **ì „í˜€ ì—†ë‹¤ë©´**, ë¹ˆ ë°°ì—´ì„ ë°˜í™˜í•˜ì„¸ìš”.  

ì˜ˆì‹œ:  
[]

===== ìƒë‹´ ëª©ë¡ =====
{formatted}
""".strip()


async def choose_best_consultation(
    user_query: str,
    consultation_results: List[Dict],
    model: str = "gpt-3.5-turbo",
) -> Dict:
    if not consultation_results:
        return {"error": "ğŸ” ê´€ë ¨ëœ ìƒë‹´ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.", "status": "no_result"}

    prompt = build_choose_one_prompt(user_query, consultation_results)

    llm = ChatOpenAI(
        model=model,
        api_key=OPENAI_API_KEY,
        temperature=0.1,
        streaming=False,
    )

    messages = [
        {
            "role": "system",
            "content": "ë‹¹ì‹ ì€ ë²•ë¥  ìƒë‹´ ë°ì´í„°ë¥¼ ì •ì œí•˜ëŠ” AI ì „ë¬¸ê°€ì…ë‹ˆë‹¤.",
        },
        {"role": "user", "content": prompt},
    ]

    response = await llm.ainvoke(messages)
    result_text = response.content

    if result_text.strip() in ["[]", "[0]"]:
        return {"error": "ğŸ™ ê´€ë ¨ëœ ìƒë‹´ì´ ì—†ìŠµë‹ˆë‹¤.", "status": "irrelevant"}

    try:
        selected = json.loads(result_text)
        selected_index = int(selected[0]) if isinstance(selected, list) else None

        if selected_index and 0 < selected_index <= len(consultation_results):
            return consultation_results[selected_index - 1]
        else:
            return {
                "error": "â— ì„ íƒëœ ì¸ë±ìŠ¤ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.",
                "status": "invalid_index",
            }

    except Exception:
        # print("âŒ ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨:", e)
        return {"error": "â— GPT ì‘ë‹µì„ ì´í•´í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", "status": "parse_error"}


# âœ… ì „ì²´ íë¦„# âœ… ì „ì²´ íë¦„ - ìˆ˜ì •ë³¸
async def run_consultation_qualifier(
    user_query: str,
    consultation_results: List[Dict],  # ì™¸ë¶€ì—ì„œ ê²€ìƒ‰ëœ ê²°ê³¼ë¥¼ ë°›ìŒ
    model: str = "gpt-3.5-turbo",
) -> Dict:
    """
    ğŸ“Œ FAISS ê¸°ë°˜ ìœ ì‚¬ ìƒë‹´ ê²€ìƒ‰ â†’ LLM ê¸°ë°˜ ê´€ë ¨ì„± íŒë‹¨ â†’ ìµœì  ìƒë‹´ ì„ íƒ íë¦„
    """

    # âŒ ì¤‘ë³µ ê²€ìƒ‰ ì œê±°
    # consultation_results, _, _ = await async_search_consultation([user_query])

    if not consultation_results:
        return {"error": "ğŸ” ê´€ë ¨ëœ ìƒë‹´ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.", "status": "no_result"}

    is_relevant = await check_relevance_to_consultations(
        user_query, consultation_results, model=model
    )
    if not is_relevant:
        return {
            "error": "ğŸ™ ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ìƒë‹´ì´ ì—†ìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ë‹¤ì‹œ ì‘ì„±í•´ë³´ì„¸ìš”.",
            "status": "no_match",
        }

    return await choose_best_consultation(user_query, consultation_results, model=model)

    # fallback ë³´ì™„: title/answer/question í‚¤ê°€ ì—†ìœ¼ë©´ fallback ê°’ í¬í•¨í•˜ì—¬ ë°˜í™˜
    # if not isinstance(result, dict) or not all(
    #     k in result for k in ["title", "question", "answer"]
    # ):
    #     print("âš ï¸ [Qualifier Fallback Triggered] ìœ íš¨í•œ ìƒë‹´ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í•¨.")
    #     return {
    #         "title": "ë²•ë¥  ì¼ë°˜",
    #         "question": user_query,
    #         "answer": "í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ ì •í™•í•œ ìƒë‹´ ë°ì´í„°ë¥¼ ì°¾ì§€ ëª»í–ˆìœ¼ë‚˜, ì¼ë°˜ì ì¸ ë²•ë¥  ì§€ì‹ì— ê¸°ë°˜í•´ ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤.",
    #         "status": "fallback_triggered",
    #     }

    # return result
